\documentclass[landscape,a0paper,fontscale=0.292]{baposter}
\usepackage[vlined]{algorithm2e}
\usepackage{times}
\usepackage{calc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage[T1]{fontenc}
	\usepackage{ae}
 \usepackage{setspace}

\usepackage{enumitem} % to reduce identation in itemize, etc

% One-lien spacing in bibliography
  \let\oldthebibliography=\thebibliography
  \let\endoldthebibliography=\endthebibliography
  \renewenvironment{thebibliography}[1]{%
    \begin{oldthebibliography}{#1}%
      \setlength{\parskip}{0ex}%
      \setlength{\itemsep}{0ex}%
  }%
  {%
    \end{oldthebibliography}%
  }



\definecolor{lightgreen}{rgb}{0,1,0}
\definecolor{lightblue}{rgb}{0.2,0.2,1}
\newcommand{\Green}[1]{\color{lightgreen}{#1}\color{black}}
\newcommand{\Blue}[1]{\color{lightblue}{#1}\color{black}}
\newcommand{\structure}[1]{{\color{blue}{#1}}}
\newcommand{\red}[1]{{\color{red}#1}}

% My commands here
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\newcommand{\insertline}{\vspace{2mm}}

\input{macros.tex}
\include{local_macros}


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % Save space in lists. Use this after the opening of the list
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newcommand{\compresslist}{%
 \setlength{\itemsep}{1pt}%
 \setlength{\parskip}{0pt}%
 \setlength{\parsep}{0pt}%
 }


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % Multicol Settings
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \setlength{\columnsep}{0.7em}
 \setlength{\columnseprule}{0mm}


\graphicspath{{./figures/}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Here starts the poster
%%---------------------------------------------------------------------------
%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{poster}{
 % Show grid to help with alignment
 grid=false,
 columns=3,
 % Column spacing
 colspacing=0.7em,
 % Color style
 headerColorOne=cyan!20!white!90!black,
 borderColor=cyan!30!white!90!black,
 % Format of textbox
%textborder=faded,
textborder=rounded,
 % Format of text header
 headerborder=open,
 headershape=roundedright,
 headershade=plain,
 background=none,
%bgColorOne=cyan!10!white,
  bgColorOne=white,
  bgColorTwo=white,
  boxColorOne=white,
 headerheight=0.12\textheight}
 % Eye Catcher
 {
     % \includegraphics[width=0.08\linewidth]{track_frame_00010_06}
 }
 % Title % FIXME: INSERT POSTER NUMBER
 {
 \sc  
	Automated Variational Inference for Gaussian Process Models
 }
  % Authors
  { 
   \vspace{5mm} 
   \smaller{ 
   \begin{tabular}{cc}
     Trung V. Nguyen &  Edwin V. Bonilla \\
      {\texttt{VanTrung.Nguyen@nicta.com.au}} &  { \texttt{e.bonilla@unsw.edu.au} } \\
     	ANU \& NICTA	&       The University of New South Wales
    \end{tabular}
   }   
 }
 %
 % University logo
 {
   \begin{tabular}{c}
   \centering
      \includegraphics[height=0.1\textheight]{logo-nicta} \hspace{5mm}
     \includegraphics[height=0.1\textheight]{logo-unsw}
   \end{tabular}
 }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Now define the boxes that make up the poster
%%%---------------------------------------------------------------------------
%%% Each box has a name and can be placed absolutely or relatively.
%%% The only inconvenience is that you can only specify a relative position 
%%% towards an already declared box. So if you have a box attached to the 
%%% bottom, one to the top and a third one which should be inbetween, you 
%%% have to specify the top and bottom boxes before you specify the middle 
%%% box.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%---------------------------------------------------------------
% Abstract
%---------------------------------------------------------------

 \headerbox{Bayesian Inference}{name=motivation,column=0,row=0,span=1}{ 
%
\textbf{Main problem:}  Trade-off flexibility $\leftrightarrow$ efficiency:
\begin{itemize}[leftmargin=0.5cm]
 \compresslist
\item 
\structure{Stochastic:} MCMC $\rightarrow$ flexible but high computational cost 
and cumbersome convergence analysis.
\item
\structure{Deterministic:} Variational, EP, Laplace $\rightarrow$  fast but derivations 
are model-specific. 
\end{itemize}
\textbf{Our  goal:} \emph{Build highly automated and efficient inference methods.}
\begin{itemize}[leftmargin=0.5cm]
\item
 \compresslist
	We focus  on Gaussian process (GP) priors and variational inference.
\end{itemize}
}
 

%---------------------------------------------------------------
% 
%---------------------------------------------------------------

\headerbox{Our Approach at a Glance}{name=approach,column=0,below=motivation}{
%
\begin{center}
	\includegraphics[width=0.8\textwidth]{avinewpdf}
\end{center}
\vspace{-0.5cm}
\textbf{Main results:}
\begin{enumerate}[leftmargin=0.5cm]
 \compresslist
\item
	Variational objective (ELBO) and its gradients are estimated efficiently: 
	expectations over \emph{univariate} Gaussians.
\item
	Efficient parameterisation of posterior covariances.
\item	
	As good as hand-coded solutions and 
%\item
	orders of magnitude faster than MCMC.
\end{enumerate}
}

%---------------------------------------------------------------
% 
%---------------------------------------------------------------

\headerbox{Latent Gaussian Process Models}{name=setting,column=0,below=approach}{
{$\bullet$ Supervised learning problems:} $\x = \{\x_n\}_{n=1}^N$, $\y = \{\y_n\}_{n=1}^N$

\insertline
{$\bullet$ Factorisation of \textbf{GP prior} over Q latent functions:}
%\begin{equation}
%f_j \sim \GP(0, \kernel_j(\cdot, \cdot))
%\rightarrow p(\f | \vtheta{0}) = \prod_{j=1}^Q p(\fj | \vtheta{0}) = \prod_{j=1}^Q \Normal(\fj; \vec{0}, \K_j) 
%\end{equation}
%\kernel_j
\vspace{-0.2cm}
\begin{center}
	\includegraphics[width=\textwidth]{prior}
\end{center}
\vspace{-0.2cm}
{$\bullet$ Factorisation of \textbf{conditional likelihood} across observations:}
%\begin{equation}
%p(\y | \f, \vtheta{1}) = \prod_{n=1}^N p(\y_n | \fn, \vtheta{1}) 
%\end{equation}
\vspace{-0.2cm}
\begin{center}
	\includegraphics[width=0.7\textwidth]{likelihood}
\end{center}
\vspace{-0.2cm}
\red{What can we model with this framework?} Multi-class classification,
multi-output regression, Warped GPs, Log Gaussian Cox Processes, and beyond
$\rightarrow$ only access to `black-box' likelihood is required.
}

%---------------------------------------------------------------
% The Model
%---------------------------------------------------------------

\headerbox{Automated  Inference Framework}{name=framework,column=1}{
\textbf{Main task:} Approximate `intractable' posterior $p(\f | \y)$

\insertline
$\bullet$   Find the closest  approximation   $p(\f | \y) \approx q(\f | \llambda)$ such that:
\begin{equation*}   
q(\f | \llambda) = \sum_{k=1}^K \pi_k  q_k(\f | \m_k, \S_k ) 
   =  \sum_{k=1}^K \ \pi_k  \prod_{j=1}^Q  \Normal(\fj; \m_{kj}, \S_{kj}) \text{,} 
\end{equation*}   
where  $\llambda = \{ \m_{kj}, \S_{kj} \} $ and 
 $q_k(\f | \m_k, \S_k)$ is the component $k$ with variational parameters 
$\m_k =  \{ \m_{kj}\}_{j=1}^{Q}$ and $\S_k = \{\S_{kj} \}_{j=1}^{Q}$. 

\insertline
\textbf{Key assumption}: \emph{Within each mixture component, the posterior 
factorises across latent functions.}

\insertline
$\bullet$ Minimize $\KL[q(\f) || p(\f | \y)]  \rightarrow$ maximize ELBO:
\begin{equation*}
\calL =  \underbrace{\expectation_q [ - \log q(\f | \llambda)] + \expectation_q  [\log p(\f)]}_{-\KL[q(\f | \llambda) || p(\f)]} +\underbrace{\sum_{k=1}^K \pi_k \expectation_{q_k}  [\log p(\y | \f)]}_{\text{ELL}}
\end{equation*}
%
We show that irrespective of the likelihood models (black-box):
\begin{enumerate}[leftmargin=0.5cm]
 \compresslist
\item
	\structure{$\KL$ term} can be bounded using Jensen's inequality: 
	Computation of exact gradients of GP hyper-parameters.
\item
	\structure{Expected log likelihood  (ELL)} term can be estimated efficiently.
\end{enumerate}
}

\newcommand{\lamk}{\llambda_{k}}
\newcommand{\lamkn}{\llambda_{k(n)}}
\newcommand{\lamki}{\llambda_{k(i)}}
\newcommand{\efi}{\f_{(i)}}
\newcommand{\gradkn}{\nabla_{\lamkn} \log q_{k(n)} (\fn | \lamkn)}
\newcommand{\elln}{\ell_n}
%---------------------------------------------------------------
% Bag of features
%---------------------------------------------------------------
\headerbox{Expected Log Likelihood Term}{name=ellterm,column=1,below=framework}{
\newtheorem{theorem1}{Theorem}
\begin{theorem1}
The expected log likelihood and its gradients can be estimated using expectations from univariate Gaussian distributions.
\end{theorem1}
(a) \structure{ELL:} Exploit the factorisation of the conditional likelihood across observations:
\vspace{-0.4cm}
\begin{equation*}
\expectation_{q_k} [\log p(\y | \f)] = \sum_{n=1}^N \expectation_{q_{k(n)}} [\underbrace{\log p(\y_n | \fn)}_{\elln}] \text{,}
\label{eq:ell}
\vspace{-0.2cm}
\end{equation*}
where $q_{k(n)} = q_{k(n)}(\fn | \lamkn)$ is the marginal posterior with variational parameters $\lamkn$ that correspond to $\fn$.
%
\begin{itemize}[leftmargin=0.5cm]
 \compresslist
\item Note that $q_{k}$ is a block-diagonal Gaussian, hence $q_{k(n)}$ is a diagonal Gaussian. 
\end{itemize}
(b) \structure{Gradients of ELL:} Use $\nabla_{x}f = f \nabla_{x} \log f, \forall f(x) > 0$:
\begin{equation*}
\nabla_{\lamkn}  \expectation_{q_{k(n)}} \elln
=  \expectation_{q_{k(n)}} \gradkn \elln.
\label{eq:ellgradients}
\end{equation*}
\textbf{Practical consequences:}
\begin{itemize}[leftmargin=0.5cm]
\compresslist
 \item
 Monte Carlo estimates can be used (or numerical integration)
%
\item
Only  evaluations of the cond.~likelihood (no gradients) are required
\end{itemize}
}


%\headerbox{KL Divergence Term}{name=klterm,column=1,below=ellterm}{
%}





%---------------------------------------------------------------
% Results
%---------------------------------------------------------------

\newcommand{\fg}{\structure{FG}}
\newcommand{\modg}{\structure{MoDG}} 
\headerbox{Practical Variational Distributions}{name=practical,column=2}{
Two posterior distribution classes of interest:
\begin{itemize}[leftmargin=0.5cm]
\compresslist
\item \fg: Full Gaussian, i.e.~K=1, full covariance matrix
%
\item \modg: Mixture of diagonal Gaussians
\end{itemize}
\newtheorem{theorem2}[theorem1]{Theorem}
\begin{theorem2}
The posterior covariance matrices can be parameterized linearly in the number 
of observations.
\end{theorem2}
This is trivial for \modg. For \fg, 
We generalise the result of  \cite{nguyen2013efficient,opper-arch-nc-2009} and show that 
the optimal posterior covariance setting is: \\
\begin{equation*}
\S_j = \big( \K_j^{-1} - 2\mat{\Lambda}_{j}) \big)^{-1} \text{,}
\end{equation*}
where $\mat{\Lambda}_{j}$ is diagonal,
hence requiring  $\calO(QN)$ parameters in total.

\textbf{Practical consequence}: Optimisation is easier, less parameters and correlations.
}



%---------------------------------------------------------------
% Experiments
%---------------------------------------------------------------

\headerbox{Experiments \& Results}{name=experiments,column=2,below=practical}{

\textbf{Binary and Multi-class Classification:} 
\begin{itemize}[leftmargin=0.5cm]
\compresslist
\item[-] Logistic sigmoid and softmax likelihoods
\end{itemize}
%
\begin{tabular}{ccc}
\includegraphics[width=0.3\linewidth]{figures/classfication-errors.eps}  &
\includegraphics[width=0.3\linewidth]{figures/breast-nlpd.eps} &
\includegraphics[width=0.3\linewidth]{figures/usps-nlpd.eps} 
\end{tabular}
\structure{\emph{ Comparable performance to hard-coded methods.}}

\insertline 
\textbf{Log Gaussian Cox process:}
\begin{itemize}[leftmargin=0.5cm]
\compresslist
\item[-] Count data: 
$p(y_n | f_n) = \frac{\lambda_n^{y_n} \exp(-\lambda_n)}{y_n!},$ where $\lambda_n = \exp(f_n + m)$ 
\item[-] Coal-mining disasters dataset 
\end{itemize}
\begin{tabular}{ccc}
\includegraphics[width=0.3\linewidth]{figures/loggcp-intensity.eps} &
\includegraphics[width=0.3\linewidth]{figures/loggcp.eps} &
\includegraphics[width=0.3\linewidth]{figures/loggcp-time.eps}
\end{tabular}
\structure{\emph{ Same performance as sampling, orders of magnitude faster.}}

\insertline
Similar results on regression and Warped GPs experiments.


}



  

%
\headerbox{References}{name=bibliography,column=2, below=experiments}{
{
\tiny
\renewcommand{\refname}{\vspace{-0.5em}}
\bibliographystyle{IEEEtran}
\bibliography{references}
%\end{spacing}
}
}


\end{poster}

\end{document}
